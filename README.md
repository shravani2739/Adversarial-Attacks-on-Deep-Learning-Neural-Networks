Overview
This project focuses on exploring and implementing adversarial attacks on deep learning models.Adversarial attacks are techniques used to fool neural networks by supplying deceptive input, often leading to incorrect or unexpected outputs. This project aims to understand these vulnerabilities and develop methods to improve the robustness of deep learning models.
Features
Implementation of various adversarial attack algorithms, including:
Fast Gradient Sign Method (FGSM)
Transferability Attack
Projected Gradient Descent (PGD)
Carlini & Wagner (C&W) Attack
DeepFool Attack
Evaluation of model robustness against adversarial attacks
Visualization tools for understanding and interpreting adversarial examples
Comprehensive documentation and examples for ease of use
